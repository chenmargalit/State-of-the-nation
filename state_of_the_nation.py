# -*- coding: utf-8 -*-
"""State of the nation

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JyLGwy5q8rjSIcz7BPGj-6wc0gjQLEok

This is not a good dataset. In some way I like working with bad datasets for educational purposes because in the real world you don't fully choose your data, sometimes the best you have is just not too good and you have to make the best of it.

### Prepare the data
"""

# Those are pretty generic imports many projects need, the rest I'll import as we go so its easir to understand where do they fit in

import numpy as np
import pandas as pd
import os
import pickle

mkdir pres

!ls

cd pres

# Read the files and put into a list

fileList = os.listdir()

data = []
path = './'
files = [f for f in os.listdir('./') if os.path.isfile(f)]
for f in files:
    with open(f,'r') as myfile:
        data.append(myfile.read())
df = pd.DataFrame(data)

data[0][:150]

# Names by order
names = ['Adams', 'Bush', 'Clinton', 'Eisenhower', 'Kennedy', 'Lincoln', 'Obama', 'Reagan', 'Roosevelt', 'Trump', 'Washington']

# Easier to work with in a DF
data = pd.DataFrame(data)
data.index = names
data.columns = ['speech']
data.head()

data_df = data.copy()
data_df.head()

data_df.to_pickle('corpus.pkl')

"""As the text consists of real presidental speeches, it's pretty clean (no typos, no spelling mistakes). For educational purposes, I will still use some cleaning that shouldn't harm the analysis. There is a balance to consider in NLP as on the one hand data does not come clean and ready, we often need to modify it to make it more comfortable to work with, more optimised (performance wise) etc. On the other hand, changing the data might very well lead to loosing important information. For example here I will uncapitalize anyting capiliazed. If I would be doing a deep neural network analysis this would maybe not make sense as capialized words might have a different meaning (e.g - this is happening now vs this is happenning NOW). As this is a part of a tool for internal use, it will fit my needs, but this remains a balance to consider

### Data cleaning
"""

# Super lightweight data cleaning as to not mess/change the data too much

import re
import string

def clean_text(text):
  text = text.lower()
  text = re.sub('title', '', text)
  # Text is full with 3 digits numbers (011, 039, 076 etc) which after going through the data, I find unhelpful to the analysis and so I decied to take it out.
  # this regular expression (e.g re) will remove every 2 (or more) digits number (10 and larger). This is quite risky normally, but going through the data,
  # and considering the nature of analysis, I think this is the right decision
  text = re.sub(r'\d\d+', '', text)
  return text

first_clean = lambda x: clean_text(x)

# Apply the cleaning
data_clean = pd.DataFrame(data.speech.apply(first_clean))

"""CountVectorizer will generate a matrix with every word in the DF and the count of times it was used by each of the speakers"""

# Create a count vectorizer containing summation of each word for each president. Here we can see 

from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer(stop_words='english')
data_cv = cv.fit_transform(data_clean.speech)
data_dtm = pd.DataFrame(data_cv.toarray(), columns = cv.get_feature_names())
data_dtm.index = data_clean.index

data_dtm.head()

data_dtm.to_pickle("dtm.pkl")

data_clean.to_pickle('data_clean.pkl')
pickle.dump(cv, open("cv.pkl", "wb"))

# Transposed version of the file, will be easier to work with in the up coming operations
data = pd.read_pickle('dtm.pkl')
data = data.transpose()
data.head()

"""### First analysis"""

# Top 30 used word for each president

top_dict = {}
for c in data.columns:
  top = data[c].sort_values(ascending=False).head(30)
  top_dict[c] = list(zip(top.index, top.values))

# Top 10

top_10 = {}
for c in data.columns:
  top = data[c].sort_values(ascending=False).head(10)
  top_10[c] = list(zip(top.index, top.values))

data = pd.read_pickle('dtm.pkl')
data = data.transpose()
data.head()

# Top 14 words for every president, same idea as the top_dict, different vizualization

for pres, value in top_dict.items():
  print(pres)
  print(', '.join([word for word, count in value[0:14]]))
  print('---')

# Lets see if some words are too much in common, as if so, they are not specific enough to any speaker

from collections import Counter
words = []
for pres in data.columns:
  top = [word for (word, count) in top_dict[pres]]
  for t in top:
    words.append(t)
words[:10]

# If the word appears in more than 6 of the speeches, lets take it off. Again, something I probably wouldn't do if this was the kind of challenege
# that I would use a deep neural network architechture for.

add_stop_words = [word for word, count in Counter(words).most_common() if count > 6]

"""Removing words like America and American is not an easy decision, nonetheless as it is very common in this kind of speech, I chose to not let it derive focus"""

add_stop_words = {'new', 'year', 'let', 'that', 've', 'just', 'american', 'america', 'americans', 'it', 'that', 'It', 'That'}

from sklearn.feature_extraction import text
from sklearn.feature_extraction.text import CountVectorizer

# Use sklearn's stop words list combined with the one I made
stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)

"""### Wordcloud"""

cv = CountVectorizer(stop_words = stop_words)
data_cv = cv.fit_transform(data_clean.speech)
data_stop = pd.DataFrame(data_cv.toarray(), columns = cv.get_feature_names())
data_stop.index = data_clean.index

# Save for later use. Saving this as pickle will make writing/reading very fast
pickle.dump(cv, open('cv_stop.pkl', 'wb'))
data_stop.to_pickle('dtm_stop.pkl')

# Prepare the wordcloud

from wordcloud import WordCloud
wc = WordCloud(stopwords = stop_words, background_color="black", colormap="Dark2",
               max_font_size=150, random_state=42)

# Commented out IPython magic to ensure Python compatibility.
# # Generate a word cloud
# 
# %%time
# import matplotlib.pyplot as plt
# 
# plt.rcParams['figure.figsize'] = [18, 9]
# 
# names = names
# 
# for index, pres in enumerate(data.columns):
#   wc.generate(data_clean.speech[pres])
#   
#   plt.subplot(3, 4, index+1)
#   plt.imshow(wc, interpolation='bilinear')
#   plt.title(names[index], color='black', size=16)
# plt.show()

"""### Second analysis"""

# How many different words does each president actually say ? 

unique_list = []
for pres in data.columns:
  unique = data[pres].to_numpy().nonzero()[0].size
  unique_list.append(unique)

unique_list

# Unique words.

data_words = pd.DataFrame(list(zip(unique_list, names)), columns=['unique_words', 'names'])
data_unique_sort = data_words.sort_values(by='unique_words', ascending=False)
data_unique_sort

# General summation of words

total_list = []
for pres in data.columns:
  total = sum(data[pres])
  total_list.append(total)

data_words['total_words'] = total_list
data_words['unique_words'] = unique_list
data_words['percent_unique_words'] = data_words['unique_words'] / data_words['total_words']

data_words

# Lets sort the data_words df by percent of unique words
data_words.index = data_df.index
data_words_by_unique_words = data_words.sort_values(by='percent_unique_words', ascending=False)

"""We see Washington has the heighest percent of unique words, lets nor forget, he's the one who said the **least** words in total, maybe its easier this way. Either way, we see its a different style of speech. We will see this more visually shortly"""

data_words_by_unique_words

total_words_index = data_words.total_words.sort_values(ascending=False).index
uniques = data_words.percent_unique_words.sort_values(ascending=False)
uniques.index

y_pos = data.columns
y_pos

data_words.head()

total_words = data_words.sort_values(by='total_words', ascending=False)
total_words.head()

plt.rcParams['figure.figsize'] = [18, 9]

plt.subplot(1, 2, 1)
plt.barh(data_words_by_unique_words.index, data_words_by_unique_words.percent_unique_words, align='center')
plt.yticks(color='black', fontsize=14)
plt.xticks(color='black', fontsize=14)
plt.title('Percent of unique words', fontsize = 20, color='black')

plt.subplot(1, 2, 2)
plt.barh(total_words.index, total_words.total_words, align='center')
plt.yticks(color='black', fontsize=14)
plt.xticks(color='black', fontsize=14)
plt.title('Number of total words', fontsize = 20, color = 'black')

plt.plot()

"""Based on own intersts and some data exploration, I'm gueesing this speech would reference (to some extent at least) issues regarding country vs economy. Let us visualize that. *pat is short for patriotism. As you probably notice, this is completely influenced by how I created the charts. Different words would provide different analysis. Therefor it's a very good idea to think what do we need and what does the data provide. As this is here for educational purposes mostly, this is fine."""

movey_vs_pat = data.transpose()[['tax', 'money', 'employment', 'wealth', 'rich', 'poor', 'economy', 'job', 'nation', 'budget', 'commerce', 'country', 'america', 'americans', 'loyalty']]
movey_vs_pat = pd.concat([movey_vs_pat.tax + movey_vs_pat.employment + movey_vs_pat.wealth + movey_vs_pat.rich + movey_vs_pat.budget +movey_vs_pat.commerce + movey_vs_pat.job + movey_vs_pat.economy + movey_vs_pat.poor, movey_vs_pat.country + movey_vs_pat.america + movey_vs_pat.nation + movey_vs_pat.loyalty], axis=1)
movey_vs_pat.columns = ['money', 'pat']
movey_vs_pat

# Trump speaks a lot about patriorism, while Obama doesn't speak about those issues almost at all (at least not in the tested vocabulary)

plt.rcParams['figure.figsize'] = [4, 8]

for i, pres in enumerate(movey_vs_pat.index):
  x = movey_vs_pat.money.loc[pres]
  y = movey_vs_pat.pat.loc[pres]
  
  plt.scatter(x, y, color='blue')
  plt.text(x+1.5, y+0.5, names[i], fontsize=10)
  plt.xlim(-5, 155)
  
plt.title('How important is money vs patriotism ? ', fontsize=20, color='black')
plt.xlabel('Money words', fontsize=15, color='black')
plt.ylabel('Pat words', fontsize=15, color='black')

plt.xticks(color='black')
plt.yticks(color='black')

from textblob import TextBlob
pol = lambda x: TextBlob(x).sentiment.polarity
sub = lambda x: TextBlob(x).sentiment.subjectivity

data_df['polarity'] = data_df['speech'].apply(pol)
data_df['subjectivity'] = data_df['speech'].apply(sub)

data_df['names'] = names
data_df.index = names
data_df.sort_values(by='subjectivity', ascending=False)

for i, pres in enumerate(data_df.index):
  y = data_df.subjectivity.loc[pres]
  print(y)

# For the second time, we see  an almost straight vector on the x axis - this is a sign of low variance in some way (neg/pos in this case)
# my guess is, reason being the context. A presidentual, state of the nation speech is far from being free everyday talk.

plt.rcParams['figure.figsize'] = [10, 8]

for index, pres in enumerate(data_df.index):
  x = data_df.polarity.loc[pres]
  y = data_df.subjectivity.loc[pres]
  
  plt.scatter(x, y, color='blue')
  plt.text(x + 0.001, y + 0.001, data_df['names'][index], fontsize=10)
  plt.xlim(0, .6)
  
plt.title('Sentiment analysis', fontsize = 20, color = 'black')
plt.xlabel('<---- Negative -------- Positive---->', fontsize = 15, color='black')
plt.ylabel('<--- Facts -------- Opinions --->', fontsize = 15, color='black')

import math

def split_text(text, n=10):
  length = len(text)
  size = math.floor(length / n)
  start = np.arange(0, length, size)
  
  split_list = []
  for piece in range(n):
    split_list.append(text[start[piece]:start[piece]+size])
  return split_list

list_pieces = []
for t in data_clean.speech:
    split = split_text(t)
    list_pieces.append(split)
    
list_pieces[0][0][:150]

polarity_speech = []
for lp in list_pieces:
  polarity_piece = []
  for word in lp:
    polarity_piece.append(TextBlob(word).sentiment.polarity)
  polarity_speech.append(polarity_piece)

data_df.head()

# How polarized is the speech ? using Textblob polarity is on scale of [-1,1]. -1 being very naegative, 1 being very positive. We see this is no memorial
# speech, its somewhat positive across speeches.

plt.rcParams['figure.figsize'] = [16, 12]

for index, comedian in enumerate(data_df.index):    
    plt.subplot(3, 4, index+1)
    plt.plot(polarity_speech[index])
    plt.plot(np.arange(0,10), np.zeros(10))
    plt.title(data_df['names'][index], color='black')
    plt.ylim(ymin=-.2, ymax=.3)

    
plt.show()

"""### Topic Modeling"""

data_df = pd.read_pickle('dtm_stop.pkl')
data_df.head()

from gensim import matutils, models
import scipy.sparse

subjects = data_df.transpose()

sparse_counts = scipy.sparse.csr_matrix(subjects)
corpus = matutils.Sparse2Corpus(sparse_counts)

cv = pickle.load(open('cv_stop.pkl', 'rb'))
# a dict with every word and the amount of times it appeared
id2word = dict((v, k) for k, v in cv.vocabulary_.items())

# passes define the amount of times the model goes through all of the data
lda = models.LdaModel(corpus = corpus, id2word = id2word, num_topics = 2, passes = 10)
lda.print_topics()

# Commented out IPython magic to ensure Python compatibility.
# # We can increase the number of topics, and the numbers of times the algorithm goes through the data
# 
# %%time
# lda = models.LdaModel(corpus = corpus, id2word = id2word, num_topics = 3, passes = 40)

lda.print_topics()

